# üéØ Sheet Name Detection - Fixed!

## Problem
Previously, sheet names in chunks were showing as "Unknown" or "example" because the simple text matching (`sheet.lower() in chunk_text.lower()`) was unreliable.

## Solution: Multi-Strategy Detection

The new `_detect_sheet_for_chunk()` function uses **4 intelligent strategies**:

### Strategy 1: Position-Based Detection ‚≠ê (Most Reliable)
- Tracks where each sheet's content appears in the extracted text
- Uses actual data values from first rows as markers
- Finds closest sheet based on character position
- **Works even when sheet names don't appear in text**

```python
# Example: If chunk is at position 1500 in text
# Sheet1 content starts at position 100
# Sheet2 content starts at position 1400
# Sheet3 content starts at position 2000
# ‚Üí Closest is Sheet2 (distance = 100) ‚úÖ
```

### Strategy 2: Sheet Name in Text
- Checks if sheet name literally appears in chunk
- Simple but effective when Tika preserves sheet names

```python
if "Requirements" in chunk_text.lower():
    return "Requirements"
```

### Strategy 3: Header Matching
- Checks if multiple column headers appear in chunk
- Requires at least 2 headers to match (avoid false positives)

```python
# If chunk contains "ID" and "Title" (2+ headers from Requirements sheet)
# ‚Üí Likely from Requirements sheet
```

### Strategy 4: Sample Value Matching
- Matches actual data values from sample rows
- Very reliable for unique values (like "REQ-001", "TC-001")
- Requires 2+ matches to confirm

```python
# If chunk contains "REQ-001" and "User Login"
# ‚Üí These are sample values from Requirements sheet
```

### Strategy 5: Default Fallback
- If all else fails, use first sheet in the file
- Better than "Unknown"

---

## How It Works

### Step 1: Build Sheet Markers (During Metadata Extraction)
```python
sheet_markers = {
    "Requirements": 150,    # Content starts at char position 150
    "TestCases": 1200,      # Content starts at char position 1200
    "Bugs": 2400            # Content starts at char position 2400
}
```

### Step 2: For Each Chunk, Detect Sheet
```python
# Chunk at position 1500
detected_sheet = _detect_sheet_for_chunk(
    chunk_text="TC-001 Test Login Pass...",
    char_position=1500,
    metadata_dict={...},
    sheet_markers={"Requirements": 150, "TestCases": 1200, "Bugs": 2400}
)
# Result: "TestCases" (closest position)
```

---

## Benefits

### ‚úÖ Before (Broken)
```json
{
  "metadata": {
    "sheet_name": "Unknown",  ‚ùå
    "chunk_index": 0
  }
}
```

### ‚úÖ After (Fixed)
```json
{
  "metadata": {
    "sheet_name": "Requirements",  ‚úÖ
    "chunk_index": 0,
    "char_position": 150,
    "header_detection": {...},
    "headers": ["ID", "Title", "Priority"]
  }
}
```

---

## Edge Cases Handled

### 1. Sheet Name Not in Text
**Problem:** Tika might not include sheet names in extracted text
**Solution:** Use position-based and value-based matching

### 2. Similar Content Across Sheets
**Problem:** Multiple sheets might have similar text
**Solution:** Require 2+ matches for header/value strategies

### 3. Single Sheet Files
**Problem:** Only one option
**Solution:** All chunks get the same (correct) sheet name

### 4. Empty Sheets
**Problem:** No content to match
**Solution:** Skip empty sheets during metadata extraction

---

## Testing

Run the test suite:
```bash
python test_sheet_detection.py
```

**Tests:**
1. ‚úÖ Multi-sheet file (Requirements, TestCases, Bugs)
2. ‚úÖ Single sheet file
3. ‚úÖ Verifies no "Unknown" sheet names

---

## Integration

The fix is automatic - no code changes needed on your end!

```python
from xlsx_converter import convert_xlsx_service

result = await convert_xlsx_service(file)

# All chunks now have correct sheet names
for chunk in result["chunks"]:
    sheet = chunk["metadata"]["sheet_name"]
    print(f"Sheet: {sheet}")  # ‚úÖ "Requirements", not "Unknown"
```

---

## For Pinecone Processing

The `excel_pinecone_processor.py` already handles sheets correctly because it:
1. Processes each sheet separately
2. Directly knows the sheet name from pandas
3. Doesn't rely on text extraction

So this fix primarily benefits the **XLSX Converter API** (`xlsx_converter.py`).

---

## Summary

**Old Approach:**
- ‚ùå Simple text search: `if sheet.lower() in text.lower()`
- ‚ùå Often returned "Unknown" or "example"
- ‚ùå Unreliable with Tika output

**New Approach:**
- ‚úÖ Position-based tracking
- ‚úÖ Multi-strategy detection (4 strategies)
- ‚úÖ Fallback chain for reliability
- ‚úÖ Always returns meaningful sheet name

**Result:** Your LLM now knows which sheet each chunk comes from! üéâ
